Gu铆a de Estudio: AWS Certified AI Practitioner - Dominio 2
Este documento resume los conceptos esenciales sobre los aspectos b谩sicos de la IA generativa, sus aplicaciones empresariales y las herramientas de AWS correspondientes, bas谩ndose en la informaci贸n de la transcripci贸n.

2.1 Conceptos B谩sicos de la IA Generativa 
Esta secci贸n cubre la definici贸n, los componentes y el funcionamiento de la inteligencia artificial generativa.

Definici贸n y Diferencias Clave
IA Generativa: Es un subconjunto del aprendizaje profundo que se enfoca en generar contenido nuevo y original (texto, im谩genes, audio, c贸digo), en lugar de solo clasificar o predecir sobre datos existentes.
IA Tradicional: Se centra principalmente en clasificar o hacer predicciones basadas en datos de entrada.
Modelos Fundacionales (FM): Son la base de la IA generativa. Son modelos de redes neuronales muy grandes y complejos, con miles de millones de par谩metros, entrenados con enormes cantidades de datos. Cuantos m谩s par谩metros, m谩s memoria y capacidad tiene el modelo para realizar tareas avanzadas.
Arquitectura Clave: El Transformador
El elemento principal de la IA generativa moderna es la red de transformadores, introducida en el art铆culo de 2017 "Attention is All You Need".

LLMs como ChatGPT se basan en esta arquitectura.
Una innovaci贸n clave es el mecanismo de autoatenci贸n, que permite al modelo ponderar la importancia de diferentes partes de la entrada, capturando as铆 relaciones contextuales y dependencias a largo plazo.
La arquitectura consiste en un codificador (que genera una representaci贸n vectorial o embedding para cada token) y un decodificador.
Terminolog铆a Esencial
Petici贸n (Prompt): La entrada (instrucciones y contenido) que se env铆a a un modelo generativo para que genere un resultado.
Inferencia: El proceso de usar un modelo entrenado para generar un resultado o "finalizaci贸n" a partir de una petici贸n.
Token: Una unidad de informaci贸n (palabra, car谩cter, etc.). El modelo usa un tokenizador para convertir texto humano en un vector de IDs de tokens.
Vectores y Embeddings: Un vector es una lista de n煤meros que representa caracter铆sticas de un concepto. Las incrustaciones (embeddings) son representaciones vectoriales que capturan el significado sem谩ntico de los tokens. Cuanto m谩s cerca est谩n dos vectores en el espacio, m谩s similar es su significado.
Aprendizaje en Contexto (In-context learning): T茅cnica para mejorar la respuesta del modelo incluyendo ejemplos de la tarea deseada directamente en la petici贸n (prompt).
Tipos de Modelos y Casos de Uso
Unimodal vs. Multimodal:

Unimodal: Trabaja con un solo tipo de datos, como los LLM que solo usan texto.
Multimodal: Procesa y genera varios tipos de datos combinados (texto, imagen, video, audio), permitiendo un razonamiento m谩s complejo.
Modelos de Difusi贸n:

Son una clase de modelos generativos que aprenden a revertir un proceso de ruido para crear un resultado de alta calidad, como una imagen.
Tienden a producir resultados de mayor calidad y son m谩s estables de entrenar que otras arquitecturas como las GANs.
Ejemplos incluyen Stable Diffusion, DALL-E y Midjourney.
Casos de Uso Principales:

Generaci贸n y resumen de texto.
Generaci贸n de c贸digo fuente para acelerar el desarrollo de software.
Extracci贸n de informaci贸n, chatbots, traducci贸n y motores de recomendaci贸n.
2.2 Capacidades y Limitaciones para Negocios 
Esta secci贸n explora las ventajas, los riesgos y las m茅tricas para evaluar la IA generativa en un contexto empresarial.

Ventajas Empresariales
Simplicidad y Accesibilidad: La IA generativa hace que muchas aplicaciones de IA sean m谩s sencillas y r谩pidas de crear a un menor coste.
Adaptabilidad: Se puede aplicar a muchas tareas y dominios diferentes sin necesidad de un reentrenamiento completo.
Limitaciones y Riesgos
Alucinaciones: El modelo puede inventar informaci贸n de forma convincente cuando no conoce la respuesta correcta. Es crucial contrastar las respuestas con fuentes autorizadas.
Contenido Da帽ino y Sesgos: Los modelos se entrenan con datos masivos de internet, por lo que pueden reproducir lenguaje t贸xico, sesgos o informaci贸n peligrosa.
Falta de Memoria: Un LLM no recuerda conversaciones anteriores por defecto; cada petici贸n es tratada de forma independiente.
Razonamiento y Matem谩ticas: Tienen una capacidad limitada para el razonamiento complejo y las matem谩ticas.
Evaluaci贸n y Selecci贸n de Modelos
M茅tricas de Evaluaci贸n:

Evaluar LLMs es complejo porque su salida no es determinista.
ROUGE (Recall-Oriented Understudy for Gisting Evaluation): Se usa para evaluar la calidad de res煤menes de texto.
BLEU (Bilingual Evaluation Understudy): Se usa para evaluar la calidad de traducciones autom谩ticas.
T茅cnicas de Mejora:

Refinamiento (Fine-tuning): Entrenar a煤n m谩s un modelo pre-entrenado con datos espec铆ficos para mejorar su rendimiento en una tarea concreta.
Aprendizaje por Refuerzo a partir de Comentarios Humanos (RLHF): Una t茅cnica de refinamiento para alinear mejor el comportamiento del modelo con las preferencias humanas y aumentar la amabilidad, honestidad e inocuidad.
M茅tricas Empresariales Clave (KPIs):

Para medir el 茅xito de una aplicaci贸n de IA, se deben usar m茅tricas como la calidad de los resultados (relevancia, exactitud), la eficiencia (tasas de finalizaci贸n de tareas), el retorno de la inversi贸n (ROI) y el valor del ciclo de vida del cliente (CLTV).
2.3 Infraestructura y Tecnolog铆as de AWS 锔
Esta secci贸n describe los servicios y la infraestructura que AWS ofrece para construir aplicaciones de IA generativa.

La Pila de IA Generativa de AWS
AWS divide su oferta en tres capas clave:

Capa Inferior (Infraestructura):

Proporciona hardware especializado para entrenamiento e inferencia, como los chips AWS Trainium y AWS Inferentia, y seguridad a nivel de hardware con AWS Nitro System.
Capa Intermedia (Herramientas y Modelos):

Permite acceder a modelos y herramientas para construir y escalar aplicaciones. Aqu铆 se encuentra Amazon SageMaker.
Capa Superior (Aplicaciones):

Incluye servicios que usan los FM para tareas espec铆ficas, como la generaci贸n de c贸digo o contenido.
Servicios Clave de AWS para IA Generativa
Amazon Bedrock:

Un servicio totalmente administrado que ofrece acceso a una variedad de modelos fundacionales (de Amazon y terceros como Cohere, Stability AI) a trav茅s de una 煤nica API.
Permite experimentar con diferentes modelos en sus "zonas de juego" (Playgrounds) para encontrar el m谩s adecuado para un caso de uso.
PartyRock es una zona de juegos de Bedrock que facilita la creaci贸n de aplicaciones de IA generativa para aprender sus fundamentos.
Amazon SageMaker JumpStart:

Act煤a como un centro de modelos (model hub) que facilita el despliegue y refinamiento de modelos fundacionales de c贸digo abierto.
Proporciona recursos como cuadernos de ejemplo y blogs para acelerar el desarrollo.
Amazon Titan:

Es la familia de modelos fundacionales creada por Amazon, disponible a trav茅s de Bedrock.
Amazon Q Developer (antes CodeWhisperer):

Un asistente de codificaci贸n con IA que genera sugerencias de c贸digo en tiempo real, desde fragmentos hasta funciones completas.
Ventajas y Consideraciones de Coste
Beneficios de AWS: Usar AWS para IA generativa ofrece ventajas como accesibilidad, rentabilidad, seguridad de nivel empresarial, conformidad y rapidez en la comercializaci贸n.

Modelos de Precios:

Alojar tu propio modelo: Implica pagar por la infraestructura de computaci贸n (GPU) y, a veces, una licencia por el modelo. Requiere inversi贸n y mantenimiento.
Pago por Token: Se paga seg煤n el n煤mero de tokens procesados (tanto en la entrada como en la salida). Es un modelo escalable ofrecido por servicios como Bedrock.
Optimizaci贸n de Costes: Es fundamental eliminar los puntos de enlace (endpoints) de los modelos de SageMaker cuando no se usan y seguir las pr谩cticas recomendadas de supervisi贸n de costes.

