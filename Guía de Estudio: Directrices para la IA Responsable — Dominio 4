# Gu√≠a de Estudio: Directrices para la IA Responsable ‚Äî Dominio 4

Este documento resume los conceptos esenciales para el desarrollo de sistemas de IA responsables, la importancia de la transparencia y las herramientas de AWS que ayudan a implementar estas pr√°cticas, bas√°ndose en la transcripci√≥n proporcionada.

## Tabla de contenidos
- [4.1 Desarrollo de Sistemas de IA Responsable üõ°Ô∏è](#41-desarrollo-de-sistemas-de-ia-responsable-üõ°Ô∏è)  
  - [El desaf√≠o del sesgo y la fiabilidad](#el-desaf√≠o-del-sesgo-y-la-fiabilidad)  
  - [Amazon SageMaker Clarify](#amazon-sagemaker-clarify)  
  - [M√©tricas de sesgo clave](#m√©tricas-de-sesgo-clave)  
  - [Riesgos espec√≠ficos de la IA generativa](#riesgos-espec√≠ficos-de-la-ia-generativa)  
  - [Herramientas de mitigaci√≥n en AWS](#herramientas-de-mitigaci√≥n-en-aws)  
- [4.2 Modelos Transparentes y Explicables üîç](#42-modelos-transparentes-y-explicables-üîç)  
  - [Fomentando la transparencia](#fomentando-la-transparencia)  
  - [Herramientas para la explicabilidad](#herramientas-para-la-explicabilidad)  
  - [Dise√±o centrado en el ser humano](#dise√±o-centrado-en-el-ser-humano)  

---

## 4.1 Desarrollo de Sistemas de IA Responsable üõ°Ô∏è

Esta secci√≥n aborda c√≥mo construir sistemas de IA justos y fiables, identificando y mitigando sesgos y riesgos.

### El desaf√≠o del sesgo y la fiabilidad
- Sesgo (Bias): desequilibrios en los datos o diferencias de rendimiento de un modelo entre distintos grupos.  
  - Un conjunto de datos desequilibrado puede provocar errores y predicciones inexactas para ciertos grupos.

### Amazon SageMaker Clarify
Amazon SageMaker Clarify es la herramienta clave de AWS para mitigar sesgos. Ayuda a:
- Detectar sesgos:
  - Antes del entrenamiento (en los datos).
  - Despu√©s del entrenamiento.
  - En modelos ya desplegados.
- Mejorar la explicabilidad: trata el modelo como "caja negra" para determinar la importancia relativa de cada caracter√≠stica en una predicci√≥n.

### M√©tricas de sesgo clave
SageMaker Clarify mide diferentes tipos de sesgo, por ejemplo:
- Disparidad demogr√°fica: indica si un grupo tiene una proporci√≥n de resultados negativos mayor que la de resultados positivos.
- Diferencia de exactitud: mide la diferencia en la precisi√≥n del modelo entre distintas clases o grupos.
- Igualdad de trato: eval√∫a si la proporci√≥n de falsos negativos y falsos positivos difiere entre clases (puede indicar sesgo aun cuando la exactitud global sea similar).

### Riesgos espec√≠ficos de la IA generativa
- Alucinaciones: el modelo inventa informaci√≥n que parece real pero es falsa; puede tener consecuencias graves (ej.: citas o hechos fabricados).
- Derechos de autor y propiedad intelectual: obras generadas por IA pueden no ser protegibles por derechos de autor; adem√°s, los modelos pueden generar contenido derivado de material protegido con el que fueron entrenados.
- Resultados discriminatorios: sesgos en los modelos pueden llevar a trato injusto y riesgos legales (ej.: sistemas de selecci√≥n de personal que discriminan por edad).
- Contenido t√≥xico: modelos pueden generar contenido ofensivo, da√±ino u obsceno si tales ejemplos estuvieron en sus datos de entrenamiento.
- Privacidad de los datos: PII u otra informaci√≥n confidencial presente en datos de entrenamiento o en peticiones puede filtrarse en las salidas del modelo.

### Herramientas de mitigaci√≥n en AWS
- Barreras de protecci√≥n en Amazon Bedrock (Guardrails):  
  - Configuraci√≥n de filtros que bloquean contenido inapropiado en peticiones y respuestas.  
  - Definici√≥n de umbrales para temas como odio, insultos, contenido sexual y violencia, o bloqueo total de determinados temas.
- Evaluaci√≥n de LLMs con SageMaker Clarify: permite comparar modelos en dimensiones clave para asegurar fiabilidad:
  - Estereotipos: probabilidad de que el modelo reproduzca sesgos (raza, g√©nero, religi√≥n, etc.).
  - Toxicidad: detecci√≥n de contenido ofensivo, grosero, odioso o agresivo.
  - Conocimiento f√°ctico: comprobaci√≥n de veracidad de las respuestas.
  - Solidez sem√°ntica: evaluaci√≥n de si peque√±as alteraciones en la entrada (errores tipogr√°ficos, espacios) cambian el resultado.
  - Exactitud: comparaci√≥n del resultado del modelo con respuestas esperadas.

---

## 4.2 Modelos Transparentes y Explicables üîç

Esta secci√≥n se centra en la importancia de entender c√≥mo funcionan los modelos de IA y en herramientas que promueven la confianza y la participaci√≥n humana.

### Fomentando la transparencia
- Software de c√≥digo abierto:  
  - El desarrollo abierto maximiza la transparencia porque el funcionamiento interno es p√∫blico.  
  - La diversidad de desarrolladores ayuda a detectar y reducir sesgos.  
  - Nota: algunas empresas limitan el uso por motivos de seguridad.
- Tarjetas de Servicio de IA de AWS (AI Service Cards):  
  - Documentos que centralizan informaci√≥n sobre el uso responsable de servicios de IA (ej.: Amazon Rekognition, Textract, Titan).  
  - Incluyen casos de uso previstos, limitaciones y mejores pr√°cticas.
- Fichas de modelo de SageMaker (SageMaker Model Cards):  
  - Permiten documentar el ciclo de vida de los modelos (detalles de entrenamiento, conjuntos de datos, contenedores usados).

### Herramientas para la explicabilidad
- SageMaker Clarify (explicabilidad):  
  - Valores de Shapley (SHAP): asignan una puntuaci√≥n a cada caracter√≠stica para determinar su contribuci√≥n a la predicci√≥n.  
  - Gr√°ficos de dependencia parcial: muestran c√≥mo var√≠an las predicciones al cambiar el valor de una caracter√≠stica espec√≠fica (p. ej., edad).

### Dise√±o centrado en el ser humano
- IA centrada en el ser humano: enfoque de dise√±o que prioriza necesidades y valores humanos; busca mejorar capacidades humanas en lugar de reemplazarlas. Involucra colaboraci√≥n multidisciplinaria (psic√≥logos, especialistas en √©tica, etc.).
- Amazon Augmented AI (A2I): servicio que incorpora revisi√≥n humana en el flujo de trabajo de la IA:
  - Permite enviar predicciones con baja confianza (o muestras aleatorias para auditor√≠a) a revisores humanos antes de entregarlas al cliente.
- Aprendizaje por Refuerzo a partir de la Retroalimentaci√≥n Humana (RLHF):  
  - T√©cnica para alinear modelos de lenguaje con preferencias humanas.  
  - Consiste en entrenar un "modelo de recompensa" con clasificaciones humanas y luego ajustar el LLM para maximizar la puntuaci√≥n de dicho modelo de recompensa.
- Amazon SageMaker Ground Truth: puede usarse para recopilar las preferencias humanas necesarias para entrenar el modelo de recompensa en RLHF.

---
