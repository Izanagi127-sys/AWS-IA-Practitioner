Gu√≠a de Estudio: AWS Certified AI Practitioner - Dominio 3
Este documento detalla la aplicaci√≥n pr√°ctica de los modelos fundacionales (FM), incluyendo consideraciones de dise√±o, ingenier√≠a de peticiones, refinamiento y evaluaci√≥n.

3.1 Consideraciones de Dise√±o de Aplicaciones con FM  (Modelos Fundacionales) ‚öôÔ∏è
Esta secci√≥n cubre los criterios para seleccionar un modelo, el impacto de los par√°metros de inferencia y las arquitecturas de dise√±o como RAG.

Criterios para Seleccionar un Modelo Previamente Entrenado
Al elegir un FM, es crucial balancear varios factores para que se ajuste a los requisitos del proyecto.


Coste vs. Rendimiento: Debes encontrar un equilibrio entre el coste de entrenamiento/mantenimiento y el rendimiento del modelo. Un modelo ligeramente menos preciso puede ser significativamente m√°s barato.



Latencia y Velocidad de Inferencia: Para aplicaciones en tiempo real (ej. veh√≠culos aut√≥nomos), la velocidad a la que un modelo genera predicciones es cr√≠tica. Modelos complejos pueden tener tiempos de inferencia mayores, lo que podr√≠a no cumplir los requisitos de latencia.




Modalidades: Considera si tu aplicaci√≥n necesita procesar un solo tipo de datos (texto) o m√∫ltiples (multimodal). Tambi√©n, si requieres capacidades multiling√ºes, debes elegir modelos entrenados en los idiomas pertinentes.



Arquitectura y Complejidad: Diferentes arquitecturas son mejores para distintas tareas (ej. CNN para im√°genes, RNN para NLP). Modelos m√°s complejos (m√°s par√°metros, m√°s capas) pueden ser m√°s precisos pero requieren m√°s recursos computacionales.




Disponibilidad y Compatibilidad: Verifica que el modelo sea compatible con tu entorno y que tenga la licencia y documentaci√≥n adecuadas. Revisa si el modelo se mantiene y actualiza regularmente.



Interpretabilidad vs. Explicabilidad: Los FM son "cajas negras" extremadamente complejas, lo que los hace no interpretables. Si la capacidad de interpretar matem√°ticamente las decisiones del modelo es un requisito, los FM podr√≠an no ser la mejor opci√≥n.


Par√°metros de Inferencia
Son valores ajustables que controlan c√≥mo el modelo genera una respuesta.

Para Aleatoriedad y Diversidad:

Temperatura: Controla la creatividad.

Top K / Top P: Limitan la selecci√≥n de tokens a los m√°s probables.

Amazon Bedrock soporta estos par√°metros para controlar la respuesta del modelo.

Para Limitar la Longitud:


Longitud de la respuesta, penalizaciones y secuencias de parada son par√°metros que se pueden usar para controlar el tama√±o del resultado generado.

Generaci√≥n Aumentada por Recuperaci√≥n (RAG)
RAG es una t√©cnica clave para mejorar los FM conect√°ndolos a bases de conocimiento externas.



¬øQu√© es? Es una t√©cnica que aumenta la petici√≥n del usuario con informaci√≥n relevante recuperada de una fuente de datos externa (como una base de datos vectorial) antes de enviarla al LLM.

¬øPor qu√© usar RAG?


Mitiga Alucinaciones: Proporciona al modelo datos f√°cticos y contextuales para basar sus respuestas, reduciendo la generaci√≥n de informaci√≥n incorrecta.



Combate el Conocimiento Desactualizado: Permite al modelo acceder a informaci√≥n actualizada sin necesidad de un costoso reentrenamiento.


Bases de Datos Vectoriales: Son un componente central de RAG. Almacenan datos (texto, im√°genes) como representaciones num√©ricas llamadas incrustaciones (embeddings). Esto permite una b√∫squeda r√°pida y eficiente de informaci√≥n sem√°nticamente similar a la consulta del usuario.




Servicios de AWS para Bases de Datos Vectoriales: Incluyen Amazon OpenSearch Service, Amazon Aurora (con la extensi√≥n pgvector), Amazon Neptune y Amazon RDS para PostgreSQL.


Agentes para Tareas con Varios Pasos

¬øQu√© son? Son programas que organizan flujos de trabajo y permiten a los FM ejecutar tareas reales que requieren interactuar con sistemas externos, como reservar un vuelo o procesar una orden de compra.



¬øC√≥mo funcionan? Desglosan tareas complejas, llaman a APIs para efectuar acciones y consultan bases de conocimiento para obtener informaci√≥n adicional.



Servicio de AWS: Agents para Amazon Bedrock es una capacidad administrada que facilita la creaci√≥n de estos agentes.

3.2 T√©cnicas de Ingenier√≠a de Peticiones (Prompt Engineering) ‚úçÔ∏è
Esta secci√≥n se enfoca en c√≥mo comunicarse eficazmente con un LLM.

Conceptos Fundamentales

Petici√≥n (Prompt): Es el conjunto de entradas que un usuario proporciona para guiar al LLM a generar una respuesta adecuada. Puede contener una instrucci√≥n, un contexto y un texto de entrada.



Espacio Latente: Es el conocimiento estad√≠stico y los patrones de lenguaje codificados dentro del modelo. Una petici√≥n consulta este espacio latente. Si el espacio latente del modelo carece de informaci√≥n sobre un tema, es m√°s probable que alucine.



T√©cnicas Comunes de Peticiones

Peticiones con Cero Muestras (Zero-shot): Se pide al modelo que realice una tarea sin darle ning√∫n ejemplo previo.


Peticiones con Pocas Muestras (Few-shot): Se proporcionan algunos ejemplos en la petici√≥n para ayudar al modelo a entender mejor la tarea y el formato de salida esperado.


Cadena de Pensamiento (Chain-of-Thought): Para tareas complejas, se le pide al modelo que divida el proceso de razonamiento en pasos intermedios, mejorando la calidad del resultado final.

Pr√°cticas Recomendadas

S√© Espec√≠fico: Proporciona instrucciones claras, incluyendo formato, estilo, tono y longitud deseada.


Usa Ejemplos: Incluye ejemplos del resultado que esperas.


Experimenta: La ingenier√≠a de peticiones es un proceso iterativo; prueba diferentes enfoques.


Conoce las Limitaciones del Modelo: Eval√∫a qu√© tan bien conoce el modelo un tema antes de crear peticiones complejas sobre √©l.

Riesgos y Mitigaci√≥n

Inyecci√≥n de Peticiones: Un ataque donde un usuario malicioso inserta instrucciones en una petici√≥n para generar una respuesta no deseada.


Jailbreaking: Intentar eludir las medidas de seguridad o barreras de protecci√≥n del modelo.


Secuestro (Hijacking): Manipular la petici√≥n original con nuevas instrucciones.


Mitigaci√≥n: Usar barreras de protecci√≥n (guardrails) para definir temas no deseados, bloquear palabras clave y filtrar contenido da√±ino o confidencial.

3.3 Entrenamiento y Refinamiento de Modelos üõ†Ô∏è
Esta secci√≥n describe c√≥mo se crean y adaptan los modelos fundacionales.

Procesos Clave

Entrenamiento Previo (Pre-training): Es el proceso inicial y masivo donde el modelo aprende de terabytes de datos no estructurados mediante aprendizaje autosupervisado. Es extremadamente costoso y requiere una enorme capacidad de c√≥mputo (millones de horas de GPU).




Refinamiento (Fine-tuning): Es un proceso de aprendizaje supervisado que adapta un modelo pre-entrenado a una tarea espec√≠fica usando un conjunto de datos etiquetados m√°s peque√±o y espec√≠fico. Ayuda a que el modelo se especialice en un dominio o caso de uso particular.


T√©cnicas de Refinamiento

Olvido Catastr√≥fico: Ocurre cuando refinar un modelo para una sola tarea hace que pierda rendimiento en otras tareas.


Refinamiento con Eficiencia de Par√°metros (PEFT): Un conjunto de t√©cnicas que congelan los par√°metros del LLM original y solo entrenan un peque√±o n√∫mero de capas o par√°metros nuevos. Esto reduce significativamente los costes de computaci√≥n y memoria. LoRA (Low-Rank Adaptation) es una t√©cnica popular de PEFT.




Adaptaci√≥n de Dominio: Refinar el modelo con datos espec√≠ficos para que aprenda la jerga de un sector o t√©rminos t√©cnicos.


Aprendizaje por Refuerzo a partir de Comentarios Humanos (RLHF): Utiliza la retroalimentaci√≥n humana para alinear mejor el modelo con las preferencias y valores humanos, mejorando la calidad de las respuestas.


Preparaci√≥n de Datos para el Refinamiento
La preparaci√≥n de datos consiste en recopilar, preprocesar y organizar los datos para el modelo.

El conjunto de datos se divide en divisiones de entrenamiento, validaci√≥n y prueba.

Servicios de AWS para Preparaci√≥n de Datos:


Amazon SageMaker Clarify: Para detectar sesgos en los datos.


Amazon SageMaker Ground Truth: Para administrar flujos de trabajo de etiquetado de datos.


Amazon SageMaker Canvas: Para flujos de datos con poco c√≥digo.

3.4 Evaluaci√≥n del Rendimiento del Modelo üìä
Esta secci√≥n cubre c√≥mo medir si un modelo fundacional es eficaz y cumple los objetivos.

Desaf√≠os de la Evaluaci√≥n
El resultado de los modelos generativos no es determinista, lo que hace que su evaluaci√≥n sea m√°s compleja que la de los modelos de ML tradicionales.

M√©tricas y Puntos de Referencia (Benchmarks)
M√©tricas Espec√≠ficas de Tareas:


ROUGE: Se usa para evaluar la calidad de res√∫menes de texto.


BLEU: Se usa para evaluar la calidad de traducciones autom√°ticas.

Benchmarks Generales:

Se utilizan para comparar el rendimiento de diferentes LLMs en una amplia gama de tareas.

Ejemplos incluyen: GLUE y SuperGLUE (comprensi√≥n general del lenguaje) , MMLU (conocimiento y resoluci√≥n de problemas en m√∫ltiples dominios) , BIG-bench (tareas que superan las capacidades actuales) y HELM (para mejorar la transparencia de los modelos).





M√©todos y Herramientas de Evaluaci√≥n

Evaluaci√≥n Humana: Utilizar personas para evaluar y comparar manualmente las respuestas de los modelos.

Herramientas de AWS:


Amazon Bedrock Model Evaluation: Permite comparar autom√°ticamente las respuestas de los modelos con una referencia humana y calcular puntuaciones (ej. BERTscore) para evaluar la fidelidad y las alucinaciones.


Amazon SageMaker Clarify: Se puede utilizar para crear trabajos de evaluaci√≥n de modelos para LLMs.

Alineaci√≥n con Objetivos Empresariales
Antes de implementar, define claramente los objetivos empresariales y las m√©tricas de √©xito.

Considera la arquitectura completa de la aplicaci√≥n, desde la infraestructura y el almacenamiento hasta la interfaz de usuario.


Es fundamental medir, supervisar y revisar continuamente las m√©tricas para evaluar si el modelo est√° cumpliendo sus objetivos y proporcionando valor.








