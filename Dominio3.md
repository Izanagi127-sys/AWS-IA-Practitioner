# Gu√≠a de Estudio: AWS Certified AI Practitioner - Dominio 3

Este documento detalla la aplicaci√≥n pr√°ctica de los modelos fundacionales (FM), incluyendo consideraciones de dise√±o, ingenier√≠a de peticiones (prompt engineering), refinamiento y evaluaci√≥n.

---

## 3.1 Consideraciones de Dise√±o de Aplicaciones con FM (Modelos Fundacionales) ‚öôÔ∏è

Esta secci√≥n cubre los criterios para seleccionar un modelo, el impacto de los par√°metros de inferencia y arquitecturas de dise√±o como RAG.

### Criterios para seleccionar un modelo previamente entrenado

Al elegir un FM, es crucial balancear varios factores para que se ajuste a los requisitos del proyecto:

- Coste vs. rendimiento  
  - Debes encontrar un equilibrio entre el coste de entrenamiento/despliegue y la precisi√≥n/ calidad. A veces un modelo menos preciso pero mucho m√°s barato y r√°pido es la opci√≥n correcta.

- Latencia y velocidad de inferencia  
  - Para aplicaciones en tiempo real (ej. veh√≠culos aut√≥nomos o chat en vivo) la velocidad es cr√≠tica. Modelos grandes pueden ofrecer mejor calidad pero con mayor latencia.

- Modalidades  
  - Considera si la aplicaci√≥n necesita una sola modalidad (texto) o varias (multimodal: texto, imagen, audio). Tambi√©n valora capacidades multiling√ºes.

- Arquitectura y complejidad  
  - Diferentes arquitecturas se adaptan mejor a distintas tareas (ej. CNN para im√°genes, transformadores para lenguaje). Modelos con m√°s par√°metros suelen requerir m√°s recursos.

- Disponibilidad y compatibilidad  
  - Verifica la licencia, documentaci√≥n, soporte y compatibilidad con tu infraestructura (frameworks, hardware).

- Interpretabilidad vs. explicabilidad  
  - Los FM suelen ser cajas negras; si necesitas interpretabilidad matem√°tica o explicaciones detalladas, eval√∫a t√©cnicas complementarias (explainers, LIME/SHAP, etc.).

### Par√°metros de inferencia

Son valores ajustables que controlan c√≥mo el modelo genera respuestas:

- Para aleatoriedad y diversidad:
  - Temperatura: controla creatividad (valores m√°s altos ‚Üí m√°s aleatorio).
  - Top-k / Top-p (nucleus sampling): limitan la selecci√≥n de tokens a los m√°s probables o al conjunto acumulado de probabilidad.
  - Amazon Bedrock y otros servicios soportan estos par√°metros para controlar la generaci√≥n.

- Para limitar la longitud:
  - M√°ximo de tokens / longitud de respuesta.
  - Penalizaciones por repetici√≥n y por longitud.
  - Secuencias de parada (stop sequences) para cortar la salida en puntos concretos.

### Generaci√≥n Aumentada por Recuperaci√≥n (RAG)

RAG mejora la fidelidad del FM conect√°ndolo a fuentes externas de conocimiento.

- ¬øQu√© es?  
  - T√©cnica que recupera informaci√≥n relevante (por ejemplo, desde una base de conocimiento vectorial) y la concatena a la petici√≥n antes de enviarla al LLM.

- ¬øPor qu√© usar RAG?
  - Mitiga alucinaciones al proporcionar evidencia factual.
  - Permite acceso a conocimiento actualizado sin reentrenar el modelo completo.

- Bases de datos vectoriales  
  - Almacenan incrustaciones (embeddings) que representan sem√°nticamente texto o contenido multimedia para b√∫squedas r√°pidas por similitud.

- Servicios de AWS que pueden usarse como parte de una soluci√≥n RAG:
  - Amazon OpenSearch Service (con plugins/vector search)
  - Amazon Aurora con extensi√≥n pgvector
  - Amazon Neptune
  - Amazon RDS para PostgreSQL (con pgvector u otras extensiones)
  - (Complementos: pipelines ETL y servicios de embeddings)

### Agentes para tareas con varios pasos

- ¬øQu√© son?  
  - Programas que organizan flujos de trabajo, usan FM para razonar y llaman a APIs o servicios para ejecutar acciones (reservar un vuelo, procesar √≥rdenes).

- ¬øC√≥mo funcionan?  
  - Dividen tareas complejas en pasos, consultan fuentes externas, llaman a sistemas externos y gestionan estado y errores.

- Servicio de AWS relacionado:  
  - Agents para Amazon Bedrock ‚Äî capacidad administrada para crear y orquestar agentes que interact√∫an con modelos y APIs externas.

---

## 3.2 T√©cnicas de Ingenier√≠a de Peticiones (Prompt Engineering) ‚úçÔ∏è

Esta secci√≥n se enfoca en c√≥mo comunicarse eficazmente con un LLM.

### Conceptos fundamentales

- Petici√≥n (prompt): conjunto de entradas que gu√≠an la respuesta del LLM (instrucciones, contexto, ejemplos).
- Espacio latente: conocimiento y patrones estad√≠sticos codificados en el modelo. La petici√≥n explora ese espacio para generar la salida.

### T√©cnicas comunes de petici√≥n

- Zero-shot: pedir al modelo realizar una tarea sin ejemplos.
- Few-shot: incluir algunos ejemplos en la petici√≥n para guiar el formato y estilo de salida.
- Chain-of-Thought (Cadena de pensamiento): pedir al modelo que describa pasos intermedios para mejorar razonamiento en tareas complejas.

### Pr√°cticas recomendadas

- S√© espec√≠fico: define formato, estilo, longitud y restricciones.
- Usa ejemplos: los ejemplos concretos suelen mejorar el resultado.
- Experimenta: la ingenier√≠a de prompts es iterativa; prueba y mide.
- Conoce las limitaciones del modelo: evita pedir tareas para las cuales el modelo no est√° bien entrenado.

### Riesgos y mitigaci√≥n

- Inyecci√≥n de peticiones: un usuario malicioso inserta instrucciones en el contexto.
- Jailbreaking: intentos de eludir restricciones del modelo.
- Secuestro (hijacking): manipular la petici√≥n original con nuevas instrucciones.

Mitigaciones:
- Implementar guardrails, filtros de contenido y saneamiento de inputs.
- Restringir y sanear datos que se incluyen en prompts.
- Validar y aplicar pol√≠ticas de seguridad y privacidad antes de ejecutar acciones cr√≠ticas.

---

## 3.3 Entrenamiento y Refinamiento de Modelos üõ†Ô∏è

Describe c√≥mo se crean y adaptan los modelos fundacionales.

### Procesos clave

- Pre-training (Entrenamiento previo): fase masiva y no supervisada donde el modelo aprende de grandes vol√∫menes de datos; es costosa en recursos.
- Fine-tuning (Refinamiento): adaptar el FM a tareas espec√≠ficas con datasets etiquetados m√°s peque√±os.

### T√©cnicas de refinamiento

- Olvido catastr√≥fico: riesgo de que al afinar para una tarea se degrade rendimiento en otras tareas.
- PEFT (Parameter-Efficient Fine-Tuning): t√©cnicas que congelan la mayor√≠a de par√°metros y entrenan solo un conjunto peque√±o (p. ej. adapters, LoRA), reduciendo costes y preservando conocimiento general.
- Adaptaci√≥n de dominio: refinar con datos espec√≠ficos del sector para que el modelo aprenda jerga y t√©rminos t√©cnicos.
- RLHF (Reinforcement Learning from Human Feedback): usar calificaciones humanas para alinear salidas del modelo con preferencias y valores humanos.

### Preparaci√≥n de datos para el refinamiento

- Recolecci√≥n y preprocesamiento: limpieza, normalizaci√≥n y anonimizaci√≥n si aplica.
- Divisiones: entrenamiento, validaci√≥n y prueba.
- Servicios de AWS para preparaci√≥n y etiquetado:
  - Amazon SageMaker Clarify: detecci√≥n de sesgos y explicabilidad.
  - Amazon SageMaker Ground Truth: flujos de trabajo de etiquetado de datos.
  - Amazon SageMaker Canvas: herramientas low-code/no-code para flujos de datos.

---

## 3.4 Evaluaci√≥n del Rendimiento del Modelo üìä

C√≥mo medir si un modelo fundacional cumple los objetivos.

### Desaf√≠os de la evaluaci√≥n

- Modelos generativos son no deterministas: evaluaci√≥n autom√°tica puede ser insuficiente.
- Necesidad de m√©tricas cualitativas y evaluaci√≥n humana para tareas abiertas.

### M√©tricas y benchmarks

- M√©tricas espec√≠ficas de tareas:
  - ROUGE: evaluaci√≥n de res√∫menes.
  - BLEU: evaluaci√≥n de traducciones.
- Benchmarks generales:
  - GLUE y SuperGLUE: comprensi√≥n del lenguaje.
  - MMLU: conocimientos y resoluci√≥n multidominio.
  - BIG-bench: tareas avanzadas que prueban l√≠mites de los modelos.

### M√©todos y herramientas de evaluaci√≥n

- Evaluaci√≥n humana: revisores que juzgan fidelidad, utilidad y seguridad.
- Herramientas de AWS:
  - Amazon Bedrock Model Evaluation: comparar respuestas de modelos con referencias humanas y calcular m√©tricas (ej. BERTScore).
  - Amazon SageMaker Clarify: soporte para evaluar aspectos relacionados con sesgos y calidad de datos/modelos.

### Alineaci√≥n con objetivos empresariales

- Define objetivos y m√©tricas de √©xito antes del despliegue.
- Considera toda la arquitectura: infraestructuras, almacenamiento, latencia, UX.
- Mide y supervisa continuamente: telemetr√≠a, alertas, pruebas A/B y revisiones peri√≥dicas.

---

Notas finales
- Antes de producci√≥n, realiza pruebas de seguridad, privacidad y cumplimiento.
- Implementa observabilidad y pipelines de retraining o actualizaci√≥n de conocimiento (por ejemplo, actualizar √≠ndices RAG).
- Mant√©n un proceso iterativo de mejora: pruebas, m√©tricas, feedback humano y refinamiento continuo.
